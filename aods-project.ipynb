{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Import Libraries & Define Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from transformers import BlipProcessor, BlipForQuestionAnswering\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Gpu Available And Moving models to GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if CUDA is available\n",
    "print(\"CUDA Available:\", torch.cuda.is_available())\n",
    "print(\"GPU Name:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"No GPU found\")\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Dataset Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define dataset paths\n",
    "TRAIN_Q_PATH = \"/kaggle/input/vqa-dataset/v2_Questions_Train_mscoco/v2_OpenEnded_mscoco_train2014_questions.json\"\n",
    "TRAIN_A_PATH = \"/kaggle/input/vqa-dataset/v2_Annotations_Train_mscoco/v2_mscoco_train2014_annotations.json\"\n",
    "VAL_Q_PATH = \"/kaggle/input/vqa-dataset/v2_Questions_Val_mscoco/v2_OpenEnded_mscoco_val2014_questions.json\"\n",
    "VAL_A_PATH = \"/kaggle/input/vqa-dataset/v2_Annotations_Val_mscoco/v2_mscoco_val2014_annotations.json\"\n",
    "\n",
    "TRAIN_IMAGE_FOLDER = \"/kaggle/input/vqa-dataset/train2014_2/train2014/\"\n",
    "VAL_IMAGE_FOLDER = \"/kaggle/input/vqa-dataset/val2014/val2014/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load JSON Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load JSON files\n",
    "def load_json(filepath):\n",
    "    with open(filepath, 'r') as f:\n",
    "        return json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert JSON data to DataFrame\n",
    "def json_to_dataframe(questions, annotations):\n",
    "    question_df = pd.DataFrame(questions['questions'])\n",
    "    annotation_df = pd.DataFrame(annotations['annotations'])\n",
    "    return pd.merge(question_df, annotation_df, on='question_id', how='inner')\n",
    "\n",
    "# Function to get image path\n",
    "def get_image_path(image_id, dataset_type=\"train\"):\n",
    "    image_filename = f\"COCO_{dataset_type}2014_{image_id:012d}.jpg\"\n",
    "    folder = TRAIN_IMAGE_FOLDER if dataset_type == \"train\" else VAL_IMAGE_FOLDER\n",
    "    return os.path.join(folder, image_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load train and validation data\n",
    "train_questions = load_json(TRAIN_Q_PATH)\n",
    "train_annotations = load_json(TRAIN_A_PATH)\n",
    "val_questions = load_json(VAL_Q_PATH)\n",
    "val_annotations = load_json(VAL_A_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert JSON to DataFrame\n",
    "train_df = json_to_dataframe(train_questions, train_annotations)\n",
    "val_df = json_to_dataframe(val_questions, val_annotations)\n",
    "\n",
    "# Rename columns and select required fields\n",
    "train_df = train_df.rename(columns={'image_id_x': 'image_id'})\n",
    "val_df = val_df.rename(columns={'image_id_x': 'image_id'})\n",
    "\n",
    "train_df = train_df[['image_id', 'question', 'multiple_choice_answer']].dropna()\n",
    "val_df = val_df[['image_id', 'question', 'multiple_choice_answer']].dropna()\n",
    "\n",
    "# Convert questions to lowercase\n",
    "train_df['question'] = train_df['question'].str.lower()\n",
    "val_df['question'] = val_df['question'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test image paths\n",
    "print(\"Sample Train Image Path:\", get_image_path(train_df.iloc[0]['image_id'], dataset_type=\"train\"))\n",
    "print(\"Sample Validation Image Path:\", get_image_path(val_df.iloc[0]['image_id'], dataset_type=\"val\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Dataset and DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VQADataset(Dataset):\n",
    "    def __init__(self, dataframe, dataset_type=\"train\"):\n",
    "        self.dataframe = dataframe\n",
    "        self.dataset_type = dataset_type\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.dataframe.iloc[idx]\n",
    "        image_id = row['image_id']\n",
    "        question = row['question']\n",
    "        answer = row['multiple_choice_answer']\n",
    "\n",
    "        image_path = get_image_path(image_id, dataset_type=self.dataset_type)\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "        # Normalize image to [0, 1] range\n",
    "        transform = transforms.Compose([\n",
    "            transforms.ToTensor(),  # Converts PIL image to tensor and scales to [0, 1]\n",
    "        ])\n",
    "        image = transform(image)\n",
    "\n",
    "        # Process image and question using the processor\n",
    "        inputs = processor(images=image, text=question, return_tensors=\"pt\", padding=True)\n",
    "        return {\n",
    "            \"inputs\": inputs,\n",
    "            \"labels\": answer\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_collate_fn(batch):\n",
    "    # Extract inputs and labels from the batch\n",
    "    inputs = [item[\"inputs\"] for item in batch]\n",
    "    answers = [item[\"labels\"] for item in batch]\n",
    "\n",
    "    # Stack images\n",
    "    images = torch.stack([input[\"pixel_values\"].squeeze(0) for input in inputs])  # Stack images\n",
    "\n",
    "    # Extract tokenized questions and pad them\n",
    "    questions = [input[\"input_ids\"].squeeze(0) for input in inputs]  # Extract tokenized questions\n",
    "    padded_questions = processor.tokenizer.pad(\n",
    "        {\"input_ids\": questions},\n",
    "        padding=True,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "    # Tokenize answers\n",
    "    tokenized_answers = processor.tokenizer(\n",
    "        answers,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"pixel_values\": images,\n",
    "        \"input_ids\": padded_questions[\"input_ids\"],\n",
    "        \"attention_mask\": padded_questions[\"attention_mask\"],  # Include attention_mask\n",
    "        \"labels\": tokenized_answers[\"input_ids\"]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dataset and DataLoader\n",
    "train_dataset = VQADataset(train_df, dataset_type=\"train\")\n",
    "val_dataset = VQADataset(val_df, dataset_type=\"val\")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, collate_fn=custom_collate_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False, collate_fn=custom_collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load BLIP Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load BLIP Model\n",
    "processor = BlipProcessor.from_pretrained(\"Salesforce/blip-vqa-base\", do_rescale=False)\n",
    "model = BlipForQuestionAnswering.from_pretrained(\"Salesforce/blip-vqa-base\").to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Validation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-02-10T22:51:32.308Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_and_validate(train_loader, val_loader, model, device, num_epochs=10, patience=5):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "    best_val_loss = float(\"inf\")\n",
    "    patience_counter = 0\n",
    "\n",
    "    # Store loss and accuracy for visualization\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    train_accuracies = []\n",
    "    val_accuracies = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_train_loss = 0\n",
    "        correct_train = 0\n",
    "        total_train = 0\n",
    "\n",
    "        for i, batch in enumerate(train_loader):\n",
    "            # Move inputs to device\n",
    "            pixel_values = batch[\"pixel_values\"].to(device)\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)  \n",
    "            labels = batch[\"labels\"].to(device)\n",
    "            \n",
    "            outputs = model(\n",
    "                pixel_values=pixel_values,\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                labels=labels\n",
    "            )\n",
    "            loss = outputs.loss\n",
    "\n",
    "            # Generate answer instead of accessing logits\n",
    "            generated_ids = model.generate(pixel_values=pixel_values, input_ids=input_ids)\n",
    "            preds = processor.tokenizer.batch_decode(generated_ids, skip_special_tokens=True)  # Convert to text\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_train_loss += loss.item()\n",
    "\n",
    "            # Calculate training accuracy\n",
    "            true_answers = processor.tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "            correct_train += sum([1 for pred, label in zip(preds, true_answers) if pred.strip().lower() == label.strip().lower()])\n",
    "            total_train += len(true_answers)\n",
    "\n",
    "            # Keep session alive - print every 10 batches\n",
    "            if i % 10 == 0:\n",
    "                print(f\"Training Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(train_loader)}], Loss: {loss.item():.4f}, Acc: {correct_train / total_train:.4f}\")\n",
    "                time.sleep(1)  # Small delay to avoid flooding logs\n",
    "\n",
    "        avg_train_loss = total_train_loss / len(train_loader)\n",
    "        train_acc = correct_train / total_train  # Training accuracy\n",
    "        train_losses.append(avg_train_loss)\n",
    "        train_accuracies.append(train_acc)\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}] | Train Loss: {avg_train_loss:.4f} | Train Acc: {train_acc:.4f}\")\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        total_val_loss = 0\n",
    "        correct_val = 0\n",
    "        total_val = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                pixel_values = batch[\"pixel_values\"].to(device)\n",
    "                input_ids = batch[\"input_ids\"].to(device)\n",
    "                attention_mask = batch[\"attention_mask\"].to(device)  \n",
    "                labels = batch[\"labels\"].to(device)\n",
    "\n",
    "                outputs = model(\n",
    "                    pixel_values=pixel_values,\n",
    "                    input_ids=input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                    labels=labels\n",
    "                )\n",
    "                loss = outputs.loss\n",
    "\n",
    "                # Generate answer instead of accessing logits\n",
    "                generated_ids = model.generate(pixel_values=pixel_values, input_ids=input_ids)\n",
    "                preds = processor.tokenizer.batch_decode(generated_ids, skip_special_tokens=True)  # Convert to text\n",
    "\n",
    "                total_val_loss += loss.item()\n",
    "\n",
    "                # Calculate validation accuracy\n",
    "                true_answers = processor.tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "                correct_val += sum([1 for pred, label in zip(preds, true_answers) if pred.strip().lower() == label.strip().lower()])\n",
    "                total_val += len(true_answers)\n",
    "\n",
    "                # Keep session alive - print every 10 batches\n",
    "                if i % 10 == 0:\n",
    "                    print(f\"Validation Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(train_loader)}], Loss: {loss.item():.4f}, Acc: {correct_train / total_train:.4f}\")\n",
    "                    time.sleep(1)  # Small delay to avoid flooding logs\n",
    "\n",
    "\n",
    "        avg_val_loss = total_val_loss / len(val_loader)\n",
    "        val_acc = correct_val / total_val  # Validation accuracy\n",
    "        val_losses.append(avg_val_loss)\n",
    "        val_accuracies.append(val_acc)\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}] | Val Loss: {avg_val_loss:.4f} | Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "        # Early Stopping\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            torch.save(model.state_dict(), \"best_model.pth\")\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(\"Early stopping triggered.\")\n",
    "                break\n",
    "\n",
    "    print(\"Training complete!\")\n",
    "\n",
    "    # Plot Training & Validation Loss and Accuracy\n",
    "    plt.figure(figsize=(12, 6))\n",
    "\n",
    "    # Plot Loss\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(range(1, len(train_losses) + 1), train_losses, label=\"Train Loss\", marker=\"o\")\n",
    "    plt.plot(range(1, len(val_losses) + 1), val_losses, label=\"Validation Loss\", marker=\"o\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(\"Training and Validation Loss\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    # Plot Accuracy\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(range(1, len(train_accuracies) + 1), train_accuracies, label=\"Train Accuracy\", marker=\"o\")\n",
    "    plt.plot(range(1, len(val_accuracies) + 1), val_accuracies, label=\"Validation Accuracy\", marker=\"o\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.title(\"Training and Validation Accuracy\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Run training\n",
    "train_and_validate(train_loader, val_loader, model, device, num_epochs=100, patience=10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 6609334,
     "sourceId": 10670998,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30840,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
